{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras-based Machine Learning Pipeline\n",
    "\n",
    "This is a pipeline for doing data analsis using the Keras package and Tensorflow.\n",
    "\n",
    "**Steps:**\n",
    "1. Load SITL reports\n",
    "2. Load MMS data\n",
    "3. Data preprocessing\n",
    "4. Train LSTM model\n",
    "\n",
    "Introduction to LSTMs and Keras: https://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Can base this on an example from Machine Learning Mastery: https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.dates import num2date, date2num\n",
    "\n",
    "# Machine learning\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import retrieve_sitl\n",
    "import retrieve_mms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load SITL data\n",
    "\n",
    "Load parsed reports in. Following would be useful:\n",
    "- Reports reduced to \"BBF\" / \"DF\" / some other interesting signals\n",
    "- Starttime and endtime and duration provided\n",
    "- If the report was for multiple events or single event or not\n",
    "- FOM value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# nothing here yet... fill me in!\n",
    "# ----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FOM</th>\n",
       "      <th>ID</th>\n",
       "      <th>Discussion</th>\n",
       "      <th>Starttime</th>\n",
       "      <th>Endtime</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>40.0</td>\n",
       "      <td>musanova(EVA)</td>\n",
       "      <td>BBFs, dipolarization front, energetic partic...</td>\n",
       "      <td>2016-05-22 09:03:34</td>\n",
       "      <td>2016-05-22 10:02:54</td>\n",
       "      <td>2016-05-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>40.0</td>\n",
       "      <td>musanova(EVA)</td>\n",
       "      <td>dipolarization front, BBFs, energetic partic...</td>\n",
       "      <td>2016-05-23 08:58:04</td>\n",
       "      <td>2016-05-23 09:16:34</td>\n",
       "      <td>2016-05-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>40.0</td>\n",
       "      <td>musanova(EVA)</td>\n",
       "      <td>BBFs, plasmasheet thinning, wave activity, en...</td>\n",
       "      <td>2016-05-24 05:50:34</td>\n",
       "      <td>2016-05-24 07:05:04</td>\n",
       "      <td>2016-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>40.0</td>\n",
       "      <td>musanova(EVA)</td>\n",
       "      <td>Dipolarization front, BBFs, energetic particl...</td>\n",
       "      <td>2016-05-25 03:57:54</td>\n",
       "      <td>2016-05-25 04:34:44</td>\n",
       "      <td>2016-05-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>30.0</td>\n",
       "      <td>musanova(EVA)</td>\n",
       "      <td>Dipolarization front, BBFs, waves, injections</td>\n",
       "      <td>2016-05-25 17:20:54</td>\n",
       "      <td>2016-05-25 17:34:04</td>\n",
       "      <td>2016-05-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>30.0</td>\n",
       "      <td>musanova(EVA)</td>\n",
       "      <td>Dipolarization front, BBF, energetic particl...</td>\n",
       "      <td>2016-05-26 06:20:44</td>\n",
       "      <td>2016-05-26 07:09:14</td>\n",
       "      <td>2016-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>20.0</td>\n",
       "      <td>musanova(EVA)</td>\n",
       "      <td>Dipolarization front, BBFs</td>\n",
       "      <td>2016-05-26 10:20:24</td>\n",
       "      <td>2016-05-26 10:47:34</td>\n",
       "      <td>2016-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>10.0</td>\n",
       "      <td>ajaynes(EVA)</td>\n",
       "      <td>Possible DFs/BBFs, particle enhancements</td>\n",
       "      <td>2016-06-11 06:11:24</td>\n",
       "      <td>2016-06-11 06:21:34</td>\n",
       "      <td>2016-06-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>10.0</td>\n",
       "      <td>ajaynes(EVA)</td>\n",
       "      <td>Tailward flows, particle enhancements, series...</td>\n",
       "      <td>2016-06-11 06:37:14</td>\n",
       "      <td>2016-06-11 06:50:54</td>\n",
       "      <td>2016-06-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>15.0</td>\n",
       "      <td>ajaynes(EVA)</td>\n",
       "      <td>BBFs/DFs, particle enhancements, nice proton ...</td>\n",
       "      <td>2016-06-11 11:39:44</td>\n",
       "      <td>2016-06-11 11:54:04</td>\n",
       "      <td>2016-06-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>20.0</td>\n",
       "      <td>ajaynes(EVA)</td>\n",
       "      <td>DFs/BBFs, particle injections, fast earthward...</td>\n",
       "      <td>2016-06-11 16:21:44</td>\n",
       "      <td>2016-06-11 16:27:44</td>\n",
       "      <td>2016-06-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>70.0</td>\n",
       "      <td>jstawarz(EVA)</td>\n",
       "      <td>Gap fill pre-BBF event.</td>\n",
       "      <td>2016-08-09 09:13:34</td>\n",
       "      <td>2016-08-09 09:15:34</td>\n",
       "      <td>2016-08-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>99.0</td>\n",
       "      <td>jstawarz(EVA)</td>\n",
       "      <td>Pre-BBF event with 300 km/s VX and field act...</td>\n",
       "      <td>2016-08-09 09:15:34</td>\n",
       "      <td>2016-08-09 09:19:04</td>\n",
       "      <td>2016-08-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>99.0</td>\n",
       "      <td>jstawarz(EVA)</td>\n",
       "      <td>Gap fill within BBF event region.</td>\n",
       "      <td>2016-08-09 09:24:14</td>\n",
       "      <td>2016-08-09 09:29:44</td>\n",
       "      <td>2016-08-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>60.0</td>\n",
       "      <td>jstawarz(EVA)</td>\n",
       "      <td>Pre-BBF event +/- 100 km/s flows. FPI data av...</td>\n",
       "      <td>2016-08-10 08:32:54</td>\n",
       "      <td>2016-08-10 08:36:44</td>\n",
       "      <td>2016-08-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>90.0</td>\n",
       "      <td>jstawarz(EVA)</td>\n",
       "      <td>Period between two BBF events. FPI data avai...</td>\n",
       "      <td>2016-08-13 08:21:54</td>\n",
       "      <td>2016-08-13 08:25:24</td>\n",
       "      <td>2016-08-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        FOM              ID  \\\n",
       "625    40.0   musanova(EVA)   \n",
       "629    40.0   musanova(EVA)   \n",
       "633    40.0   musanova(EVA)   \n",
       "637    40.0   musanova(EVA)   \n",
       "639    30.0   musanova(EVA)   \n",
       "641    30.0   musanova(EVA)   \n",
       "642    20.0   musanova(EVA)   \n",
       "831    10.0    ajaynes(EVA)   \n",
       "833    10.0    ajaynes(EVA)   \n",
       "838    15.0    ajaynes(EVA)   \n",
       "839    20.0    ajaynes(EVA)   \n",
       "1192   70.0   jstawarz(EVA)   \n",
       "1193   99.0   jstawarz(EVA)   \n",
       "1195   99.0   jstawarz(EVA)   \n",
       "1205   60.0   jstawarz(EVA)   \n",
       "1245   90.0   jstawarz(EVA)   \n",
       "\n",
       "                                             Discussion           Starttime  \\\n",
       "625     BBFs, dipolarization front, energetic partic... 2016-05-22 09:03:34   \n",
       "629     dipolarization front, BBFs, energetic partic... 2016-05-23 08:58:04   \n",
       "633    BBFs, plasmasheet thinning, wave activity, en... 2016-05-24 05:50:34   \n",
       "637    Dipolarization front, BBFs, energetic particl... 2016-05-25 03:57:54   \n",
       "639       Dipolarization front, BBFs, waves, injections 2016-05-25 17:20:54   \n",
       "641     Dipolarization front, BBF, energetic particl... 2016-05-26 06:20:44   \n",
       "642                          Dipolarization front, BBFs 2016-05-26 10:20:24   \n",
       "831            Possible DFs/BBFs, particle enhancements 2016-06-11 06:11:24   \n",
       "833    Tailward flows, particle enhancements, series... 2016-06-11 06:37:14   \n",
       "838    BBFs/DFs, particle enhancements, nice proton ... 2016-06-11 11:39:44   \n",
       "839    DFs/BBFs, particle injections, fast earthward... 2016-06-11 16:21:44   \n",
       "1192                            Gap fill pre-BBF event. 2016-08-09 09:13:34   \n",
       "1193    Pre-BBF event with 300 km/s VX and field act... 2016-08-09 09:15:34   \n",
       "1195                  Gap fill within BBF event region. 2016-08-09 09:24:14   \n",
       "1205   Pre-BBF event +/- 100 km/s flows. FPI data av... 2016-08-10 08:32:54   \n",
       "1245    Period between two BBF events. FPI data avai... 2016-08-13 08:21:54   \n",
       "\n",
       "                 Endtime         Day  \n",
       "625  2016-05-22 10:02:54  2016-05-22  \n",
       "629  2016-05-23 09:16:34  2016-05-23  \n",
       "633  2016-05-24 07:05:04  2016-05-24  \n",
       "637  2016-05-25 04:34:44  2016-05-25  \n",
       "639  2016-05-25 17:34:04  2016-05-25  \n",
       "641  2016-05-26 07:09:14  2016-05-26  \n",
       "642  2016-05-26 10:47:34  2016-05-26  \n",
       "831  2016-06-11 06:21:34  2016-06-11  \n",
       "833  2016-06-11 06:50:54  2016-06-11  \n",
       "838  2016-06-11 11:54:04  2016-06-11  \n",
       "839  2016-06-11 16:27:44  2016-06-11  \n",
       "1192 2016-08-09 09:15:34  2016-08-09  \n",
       "1193 2016-08-09 09:19:04  2016-08-09  \n",
       "1195 2016-08-09 09:29:44  2016-08-09  \n",
       "1205 2016-08-10 08:36:44  2016-08-10  \n",
       "1245 2016-08-13 08:25:24  2016-08-13  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... But for now, here's a basic approach:\n",
    "pickle_path = \"../pydata/reports_df.p\"   # pickle created in example_sitl_read.ipynb\n",
    "reports_df = pd.read_pickle(pickle_path)\n",
    "reports_df = retrieve_sitl.parse_times(reports_df)\n",
    "reports_df = reports_df.drop(columns='datetime')\n",
    "reports_df = retrieve_sitl.combine_rows(reports_df)\n",
    "\n",
    "# Pick out reports from 2016 with BBFs:\n",
    "reports_df_2016 = reports_df.loc[np.logical_and(reports_df['Day'] >= '2016-01-01', reports_df['Day'] < '2017-01-01')]\n",
    "reports_df_2016.loc[reports_df_2016.Discussion.str.contains('BBF',case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FOM                                                        40.0\n",
       "ID                                                musanova(EVA)\n",
       "Discussion      BBFs, dipolarization front, energetic partic...\n",
       "Starttime                                   2016-05-22 09:03:34\n",
       "Endtime                                     2016-05-22 10:02:54\n",
       "Day                                                  2016-05-22\n",
       "Name: 625, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's an example event, but ideally we would have a list of these events:\n",
    "event = reports_df_2016.loc[625]\n",
    "event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load MMS data\n",
    "\n",
    "Load in raw data for processing.\n",
    "\n",
    "Load both magnetic (FGM) and plasma/ion data (FPI, not yet implemented). Both will be useful to feed into a machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file at ../pydata/mms1/fgm/srvy/l2/2016/05/mms1_fgm_srvy_l2_20160522_v4.40.0.cdf\n"
     ]
    }
   ],
   "source": [
    "starttime, endtime = datetime(2016,5,22), datetime(2016,5,23)\n",
    "local_data_dir = '../pydata'\n",
    "base_path = os.path.join(local_data_dir, 'mms1', 'fgm', 'srvy', 'l2')\n",
    "data = retrieve_mms.get_fgm(base_path, starttime, endtime)\n",
    "t_utc, B_x, B_y, B_z, Bt = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also read FPI data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data proprocessing\n",
    "\n",
    "First steps...\n",
    "\n",
    "1. Resample data to 1s? I don't think we need a higher resolution. Maybe somebody else knows better here.\n",
    "2. Label data with classes, e.g. all data has class 0 to start with, data marked as BBF in reports is then labelled with class 1, then data marked as DF is labelled with class 2, etc. We can try to predict the event type contained in a short time series within the data.\n",
    "3. May want to remove general trends so data is static. I think the signals we're looking for are only related to variations, not to overall increase/decrease in magnetic field values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample data:\n",
    "# --------------\n",
    "t_sec = np.array([starttime + timedelta(seconds=n) for n in range(int((endtime-starttime).total_seconds()))])\n",
    "t_sec_num = date2num(t_sec)\n",
    "t_utc_num = date2num(t_utc)\n",
    "# Interpolate to new time array:\n",
    "B_x_sec = np.interp(t_sec_num, t_utc_num, B_x)\n",
    "B_y_sec = np.interp(t_sec_num, t_utc_num, B_y)\n",
    "B_z_sec = np.interp(t_sec_num, t_utc_num, B_z)\n",
    "Bt_sec = np.interp(t_sec_num, t_utc_num, Bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in array: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Create arrays with data classes:\n",
    "# --------------------------------\n",
    "t_classes = np.zeros(len(t_sec))\n",
    "t_classes[np.logical_and(t_sec >= event.Starttime, t_sec < event.Endtime)] = 1\n",
    "print(\"Values in array:\", np.unique(t_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps in preparation for machine learning\n",
    "\n",
    "Combine all MMC variables into one array, and then...\n",
    "\n",
    "1. Split MMS data into 30-minute sections (samples).\n",
    "2. Give each section a class if it contains an interesting signal. Use keras.utils.to_categorical() on this.\n",
    "3. Scale input data for LSTM (sklearn.MixMaxScaler).\n",
    "4. Make a train-test split of all data.\n",
    "5. Input data (X) should be n-dimensional and contain magnetic field variations and ion data. **What else can we add to this?** Output data (y) should be 1-dimensional and contain one class (int value) per 30-minute section. It's a good place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ -66.12447357,  -66.12447357,  -66.12447357, ..., -107.55329567,\n",
       "        -107.5307406 , -107.45926477]),\n",
       " array([-107.42327801, -107.33263814, -107.26846468, ..., -103.12379227,\n",
       "        -103.11797762, -103.1142554 ]),\n",
       " array([-103.08862392, -103.05110097, -102.97588577, ...,  -92.99384607,\n",
       "         -93.00339458,  -92.98151217]),\n",
       " array([-92.96826781, -92.90722263, -92.8760217 , ..., -78.25201963,\n",
       "        -78.21153439, -78.16517996]),\n",
       " array([-78.10604359, -78.08643259, -78.05366718, ..., -71.5058605 ,\n",
       "        -71.52721376, -71.53060211]),\n",
       " array([-71.54413859, -71.50295362, -71.46344387, ..., -64.15184825,\n",
       "        -64.18456652, -64.19085939]),\n",
       " array([-64.17307514, -64.17144794, -64.13855006, ..., -68.25921929,\n",
       "        -67.91225867, -67.64160247]),\n",
       " array([-67.52788397, -67.60527692, -67.87689872, ..., -64.60936155,\n",
       "        -64.63435936, -64.64636287]),\n",
       " array([-64.60705909, -64.62022575, -64.61281618, ..., -62.93120145,\n",
       "        -63.20035394, -63.22907614]),\n",
       " array([-63.33672004, -63.38201676, -63.44965501, ..., -51.70437548,\n",
       "        -51.71419754, -51.71325589]),\n",
       " array([-51.73379609, -51.72338609, -51.71443606, ..., -50.73764719,\n",
       "        -50.77205248, -50.78467103]),\n",
       " array([-50.81135832, -50.83531597, -50.90831231, ..., -46.9347662 ,\n",
       "        -46.91896277, -46.90944022]),\n",
       " array([-46.8891123 , -46.87945002, -46.90559034, ..., -43.47027122,\n",
       "        -43.45709621, -43.46232049]),\n",
       " array([-43.45975847, -43.49208857, -43.52953215, ..., -40.94155262,\n",
       "        -40.93978913, -40.98985552]),\n",
       " array([-40.99962987, -41.01158608, -41.00030803, ..., -42.75907726,\n",
       "        -42.72472659, -42.72744584]),\n",
       " array([-42.73043797, -42.72992628, -42.73576226, ..., -45.41017342,\n",
       "        -45.4173839 , -45.406903  ]),\n",
       " array([-45.40023654, -45.38834528, -45.36990532, ..., -42.07602003,\n",
       "        -42.09386029, -42.09050629]),\n",
       " array([-42.08865805, -42.09201921, -42.10864482, ..., -41.10580559,\n",
       "        -41.06607037, -41.08145714]),\n",
       " array([-41.07885213, -41.07185251, -41.00679447, ..., -42.88857544,\n",
       "        -42.90763301, -42.90134582]),\n",
       " array([-42.9235896 , -42.94540818, -42.93407122, ..., -28.42989324,\n",
       "        -28.45763512, -28.46962023]),\n",
       " array([-28.51404986, -28.56492058, -28.62823399, ..., -29.54965176,\n",
       "        -29.58117116, -29.59277907]),\n",
       " array([-29.61751985, -29.60870071, -29.63576743, ..., -31.19531365,\n",
       "        -31.18357284, -31.15689821]),\n",
       " array([-31.19578681, -31.17771422, -31.16554513, ..., -34.69461881,\n",
       "        -34.68089644, -34.67442387]),\n",
       " array([-34.65969871, -34.69671091, -34.67836778, ..., -36.56383678,\n",
       "        -36.55758345, -36.5457661 ]),\n",
       " array([-36.55474009, -36.55902603, -36.56202699, ..., -38.2634535 ,\n",
       "        -38.20306487, -38.18961122]),\n",
       " array([-38.21282659, -38.22329676, -38.15858326, ..., -33.45238129,\n",
       "        -33.4472884 , -33.5016906 ]),\n",
       " array([-33.50940726, -33.55512782, -33.58995224, ..., -33.80055543,\n",
       "        -33.81298779, -33.80700147]),\n",
       " array([-33.80888231, -33.82402439, -33.83925424, ..., -35.54303469,\n",
       "        -35.55537022, -35.51395999]),\n",
       " array([-35.493009  , -35.45995352, -35.47113432, ..., -36.06350061,\n",
       "        -36.04006965, -36.00686293]),\n",
       " array([-35.9851291 , -35.95191518, -35.9493826 , ..., -37.0621782 ,\n",
       "        -37.04316098, -37.0380089 ]),\n",
       " array([-37.02188711, -37.03776686, -37.02338586, ..., -36.91215918,\n",
       "        -36.91097907, -36.90547608]),\n",
       " array([-36.89720456, -36.88610741, -36.93465145, ..., -41.41010982,\n",
       "        -41.40684821, -41.41607411]),\n",
       " array([-41.40740542, -41.42112715, -41.44270068, ..., -43.02762004,\n",
       "        -43.0127228 , -42.92146744]),\n",
       " array([-42.85981597, -42.84303546, -42.80891485, ..., -46.64538114,\n",
       "        -46.61940709, -46.67078471]),\n",
       " array([-46.71050639, -46.63533753, -46.5549384 , ..., -44.26491893,\n",
       "        -44.2222278 , -44.31569097]),\n",
       " array([-44.41378054, -44.42698802, -44.40837472, ..., -49.88164461,\n",
       "        -49.87698343, -49.87250795]),\n",
       " array([-49.86315459, -49.82754072, -49.82528878, ..., -49.10761547,\n",
       "        -49.09164252, -49.11159415]),\n",
       " array([-49.08345432, -49.06501003, -49.09491401, ..., -51.49906023,\n",
       "        -51.508091  , -51.48465314]),\n",
       " array([-51.48614374, -51.49361357, -51.49336985, ..., -54.46235091,\n",
       "        -54.4751424 , -54.4839191 ]),\n",
       " array([-54.50517429, -54.49119402, -54.49546386, ..., -55.38403186,\n",
       "        -55.42948715, -55.40828583]),\n",
       " array([-55.39874299, -55.42172093, -55.40074429, ..., -49.63468454,\n",
       "        -49.61017189, -49.59026665]),\n",
       " array([-49.57767508, -49.55425483, -49.5345443 , ..., -16.18411788,\n",
       "        -16.15421542, -16.12655387]),\n",
       " array([-16.07392775, -16.0516049 , -16.05207911, ..., 143.58339896,\n",
       "        143.75055664, 143.89793283]),\n",
       " array([144.08681838, 144.24295408, 144.40453315, ..., 969.67775124,\n",
       "        970.56744365, 971.405943  ]),\n",
       " array([  972.25456536,   973.1540983 ,   973.97864788, ...,\n",
       "        -5952.31622201, -5893.52636224, -7535.37188613]),\n",
       " array([-8627.84328392, -7195.28770597, -6035.34964751, ...,\n",
       "         2303.18948735,  2303.12189585,  2302.99698943]),\n",
       " array([2302.98368143, 2302.90828328, 2302.89011422, ...,  222.14400918,\n",
       "         221.70300761,  221.25535953]),\n",
       " array([220.84117008, 220.42227268, 220.01493611, ..., -80.58530392,\n",
       "        -80.65751829, -80.73273624])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lots of preprocessing to do...\n",
    "\n",
    "# example of splitting array into chunks\n",
    "test = np.array_split(B_x_sec, 24*2)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train LSTM model\n",
    "\n",
    "Take this as starting example: https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing here yet..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply trained model\n",
    "\n",
    "Once model is trained, can we use it to find similar signals in data outside what we've trained it on? Would be cool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# would be nice..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
